---
title: "TP n°3 : Support Vector Machine (SVM)"
author: "Lucine Bonnefont"
format:
  pdf:
    latex-engine: xelatex
execute: 
  echo: false
---


# Objectif
L’objectif de ce TP est de mettre en œuvre la méthode de classification SVM, aussi bien sur des données réelles que simulées, à l’aide de la bibliothèque scikit-learn. Il s’agit également d’apprendre à ajuster ses paramètres (hyperparamètres et choix du noyau) afin d’en maîtriser la flexibilité.

# Mise en oeuvre
```{python}
import sys
sys.path.append("../code")

from svm_source import *
```

```{python}
import numpy as np
import matplotlib.pyplot as plt
from sklearn.svm import SVC
from sklearn import svm
from sklearn import datasets
from sklearn.utils import shuffle
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.datasets import fetch_lfw_people
from sklearn.decomposition import PCA
from time import time

scaler = StandardScaler()

import warnings
warnings.filterwarnings("ignore")

plt.style.use('ggplot')
```

## Dataset "Iris"

Dans cette expérience, l’objectif était de tester un classifieur à vecteurs de support (SVM) linéaire sur le jeu de données Iris, en se concentrant uniquement sur les classes 1 et 2, c’est-à-dire versicolor et virginica. Le dataset Iris contient en réalité trois classes, mais on a volontairement écarté la classe 0 (setosa) car elle est trop facilement séparée. De plus, on n’a retenu que les deux premières variables explicatives, la longueur et la largeur des sépales, afin de travailler dans un espace bidimensionnel et de visualiser plus clairement les limites du classifieur linéaire.

```{python}
iris = datasets.load_iris()
X = iris.data
X = scaler.fit_transform(X)
y = iris.target
X = X[y != 0, :2] # ne prend que les  target =! 0 et les 2 premières caractéristiques
y = y[y != 0]

def f_linear(xx):
    return clf_linear.predict(xx.reshape(1, -1))

def f_poly(xx):
    return clf_poly.predict(xx.reshape(1, -1))

plt.figure(figsize=(15, 5))
plot_2d(X, y)
plt.title("iris dataset")
```

Pour ajuster le paramètre de régularisation C, qui contrôle l’équilibre entre la largeur de la marge et le respect strict des données d’apprentissage, une recherche sur une grille de valeurs a été menée grâce à la fonction *GridSearchCV*.

```{python}

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

parameters = {'kernel': ['linear'], 'C': list(np.logspace(-3, 3, 200))}
clf2 = SVC()
clf_linear = GridSearchCV(clf2, parameters, n_jobs=-1)
clf_linear.fit(X_train, y_train)

# check your score
print(clf_linear.best_params_)

# compute the score

print('Generalization score for linear kernel: %s, %s' %
      (clf_linear.score(X_train, y_train),
       clf_linear.score(X_test, y_test)))
```

 
Après entraînement, le meilleur paramètre trouvé est $C=0,84$. Avec cette valeur, le score obtenu sur l’ensemble d’apprentissage est d’environ 0,75 et de 0,68 pour l'ensemble de test.

Ces résultats signifient que le modèle parvient à faire mieux que le hasard (qui donnerait un taux proche de 0,5) mais reste assez limité. La raison principale tient au fait que les classes versicolor et virginica ne sont pas linéairement séparables. Les nuages de points se chevauchent fortement, si bien qu’aucun hyperplan ne peut tracer une frontière claire entre les deux espèces. 

En espérant améliorer la performence du modèle nous allons utiliser un noyau polynomial cette fois ci.

```{python}
Cs = list(np.logspace(-3, 3, 5))
gammas = 10. ** np.arange(1, 2)
degrees = np.r_[1, 2, 3]

# fit the model and select the best set of hyperparameters
parameters = {'kernel': ['poly'], 'C': Cs, 'gamma': gammas, 'degree': degrees}
clf2 = SVC()
clf_poly = GridSearchCV(clf2, parameters, n_jobs=-1)
clf_poly.fit(X_train, y_train)

print(clf_poly.best_params_)
print('Generalization score for polynomial kernel: %s, %s' %
      (clf_poly.score(X_train, y_train),
       clf_poly.score(X_test, y_test)))

```

Nous retrouvons la même performence que lors de l'utilisation de noyau linéaire. Cette égalité des performances s’explique par le fait qu’un noyau polynomial de degré 1 est en réalité équivalent à un noyau linéaire. Ainsi, pour nos données, la frontière de séparation linéaire semble suffisante et l’usage d’un noyau polynomial plus complexe n’apporte aucun avantage.

```{python}
plt.figure(figsize=(15, 5))

plt.subplot(121)
frontiere(f_linear, X, y)
plt.title("linear kernel")

plt.subplot(122)
frontiere(f_poly, X, y)

plt.title("polynomial kernel")
plt.tight_layout()
plt.draw()
```



## SVM GUI

Cette application permet d’évaluer l’impact du choix du paramètre de régularisation C, qui contrôle le compromis entre la maximisation de la marge et la minimisation de l’erreur de classification, appliqué ici à un jeu de données déséquilibré.

Lorsque C est grand, le SVM cherche à classer correctement tous les points, y compris ceux de la classe minoritaire, quitte à réduire la largeur de la marge. Ce choix peut conduire à une frontière de décision complexe et à un risque de surapprentissage.

À l’inverse, lorsque C est faible, le SVM accepte davantage d’erreurs afin de maximiser la largeur de la marge. Dans un contexte déséquilibré, cela se traduit souvent par une frontière qui favorise la classe majoritaire, car les points minoritaires sont davantage ignorés. Ce modèle est toutefois considéré comme plus robuste au bruit.

## Classification de visages

- Lors de l’expérimentation avec un noyau linéaire, nous avons étudié l’influence du paramètre de régularisation C sur la classification des visages de Tony Blair et de Colin Powell. Après apprentissage, le meilleur compromis est obtenu pour une valeur de C égale à 0,001, avec un score de 0,91 sur l’ensemble de test. Ce résultat est largement supérieur au niveau de hasard estimé à 0,62, ce qui confirme la capacité du modèle à apprendre une séparation pertinente entre les deux classes. L’exécution du modèle s’est révélée rapide, avec un temps de calcul inférieur à une seconde, et la précision obtenue illustre l’importance du réglage du paramètre C pour atteindre de bonnes performances tout en évitant le surapprentissage.

```{python}
from sklearn.datasets import fetch_lfw_people
import time

lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4,
                              color=True, funneled=False, slice_=None,
                              download_if_missing=True)
# data_home='.'

# introspect the images arrays to find the shapes (for plotting)
images = lfw_people.images
n_samples, h, w, n_colors = images.shape

# the label to predict is the id of the person
target_names = lfw_people.target_names.tolist()

####################################################################
# Pick a pair to classify such as
names = ['Tony Blair', 'Colin Powell']
# names = ['Donald Rumsfeld', 'Colin Powell']

idx0 = (lfw_people.target == target_names.index(names[0]))
idx1 = (lfw_people.target == target_names.index(names[1]))
images = np.r_[images[idx0], images[idx1]]
n_samples = images.shape[0]
y = np.r_[np.zeros(np.sum(idx0)), np.ones(np.sum(idx1))].astype(int)

X = (np.mean(images, axis=3)).reshape(n_samples, -1)

# # or compute features using colors (3 times more features)
# X = images.copy().reshape(n_samples, -1)

# Scale features
X -= np.mean(X, axis=0)
X /= np.std(X, axis=0)

# ---- Ajout de la graine ici ----
rng = np.random.RandomState(42)
indices = rng.permutation(X.shape[0])

train_idx, test_idx = indices[:X.shape[0] // 2], indices[X.shape[0] // 2:]
X_train, X_test = X[train_idx, :], X[test_idx, :]
y_train, y_test = y[train_idx], y[test_idx]
images_train, images_test = images[
    train_idx, :, :, :], images[test_idx, :, :, :]

print("--- Linear kernel ---")
print("Fitting the classifier to the training set")
t0 = time.time()

# fit a classifier (linear) and test all the Cs
Cs = 10. ** np.arange(-5, 6)
scores = []

for C in Cs:
    clf = SVC(C=C, kernel="linear", random_state=42)  # ajout graine
    clf.fit(X_train, y_train)
    scores.append(clf.score(X_test, y_test))

ind = np.argmax(scores)
print("Best C: {}".format(Cs[ind]))

plt.figure()
plt.plot(Cs, scores)
plt.xlabel("Parametres de regularisation C")
plt.ylabel("Scores d'apprentissage")
plt.xscale("log")
plt.tight_layout()
plt.show()
print("Best score: {}".format(np.max(scores)))

print("Predicting the people names on the testing set")
t0 = time.time()

clf = SVC(C=Cs[ind], kernel="linear", random_state=42)  # ajout graine
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)

print("done in %0.3fs" % (time.time() - t0))
# The chance level is the accuracy that will be reached when constantly predicting the majority class.
print("Chance level : %s" % max(np.mean(y), 1. - np.mean(y)))
print("Accuracy : %s" % clf.score(X_test, y_test))

```

Lorsque le paramètre de régularisation C est très faible, par exemple de l’ordre de $10^{-5}$, le SVM privilégie une marge très large et tolère de nombreuses erreurs de classification, ce qui entraîne une frontière de décision trop éloignée des points et une faible précision. Dès que C augmente légèrement, le modèle devient plus strict dans la classification des points d’entraînement, ce qui améliore rapidement la séparation entre les deux classes et stabilise la performance à un niveau élevé. Ainsi, la valeur de C contrôle bien l’équilibre entre la largeur de la marge et la fidélité aux données, et un réglage trop faible empêche le modèle de capturer correctement les distinctions entre les visages.



-  Nous avons ensuite étudié l’impact de l’ajout de variables de nuisance sur la performance d’un SVM linéaire. Sans variables de nuisance, le modèle atteint un score parfait sur l’ensemble d’entraînement et une précision de 0,91 sur l’ensemble de test, ce qui montre que le SVM est capable de séparer efficacement les visages de Tony Blair et Colin Powell.
-  
En ajoutant 300 variables aléatoires supplémentaires, indépendantes de la cible, la performance sur l’ensemble de test chute drastiquement à 0,53, tandis que le score sur l’ensemble d’entraînement reste à 1,0. Cela illustre que l’introduction de variables non informatives augmente la complexité du modèle et provoque un surapprentissage, réduisant sa capacité de généralisation. L’expérience met donc en évidence la sensibilité des SVM linéaires à la présence de variables inutiles lorsque le nombre de caractéristiques augmente par rapport au nombre d’exemples.


```{python}
# Création d'un générateur aléatoire pour fixer la graine
rng = np.random.RandomState(42)

def run_svm_cv(_X, _y):
    _indices = rng.permutation(_X.shape[0])  # utilisation de la graine
    _train_idx, _test_idx = _indices[:_X.shape[0] // 2], _indices[_X.shape[0] // 2:]
    _X_train, _X_test = _X[_train_idx, :], _X[_test_idx, :]
    _y_train, _y_test = _y[_train_idx], _y[_test_idx]

    _parameters = {'kernel': ['linear'], 'C': list(np.logspace(-3, 3, 5))}
    _svr = svm.SVC(random_state=42)  # ajout de la graine
    _clf_linear = GridSearchCV(_svr, _parameters)
    _clf_linear.fit(_X_train, _y_train)

    print('Generalization score for linear kernel: %s, %s \n' %
          (_clf_linear.score(_X_train, _y_train), _clf_linear.score(_X_test, _y_test)))

print("Score sans variable de nuisance")
run_svm_cv(X, y)

print("Score avec variable de nuisance")
n_features = X.shape[1]
# On rajoute des variables de nuisances
sigma = 1
noise = sigma * rng.randn(n_samples, 300)  # utilisation de la graine
X_noisy = np.concatenate((X, noise), axis=1)
X_noisy = X_noisy[rng.permutation(X.shape[0])]  # utilisation de la graine
run_svm_cv(X_noisy, y)

```


-   Après application d’une réduction de dimension par PCA, la performance du SVM linéaire sur les données bruitées est légèrement diminuée sur l’ensemble d’entraînement et reste faible sur l’ensemble de test, même si elle semble améliorer légèrement la performence. Cette expérience devrait montrer que la PCA permet de compresser l’information et de réduire la complexité liée aux variables de nuisance en conservant les composantes principales les plus informatives. Dans notre cas, nous ne pouvons pas réduire conséquemment le nombre de composantes principales par problème de coût de calcul, ce qui explique ce score très proche de celui avec l'ajout de bruit, encore beaucoup de composantes conservées ne sont que du bruit.

```{python}
print("Score après réduction de dimension")

n_components = 150 
pca = PCA(n_components=n_components, svd_solver='randomized', random_state=42).fit(X_noisy)
X_noisy_pca = pca.transform(X_noisy)
run_svm_cv(X_noisy_pca, y)

```


-  Lors du prétraitrement des données on fait la moyenne sur les canaux de couleur(RGB) ce qui entraine une réduction de l’image à une seule intensité par pixel, en supprimant toute information relative à la couleur. Certaines différences pouvant être discriminantes entre les visages (peau, cheveux, vêtements) sont perdues. Cela peut biaiser le modèle vers les traits de luminosité uniquement.